import streamlit as st
import requests
from utils import load_all_excels, semantic_search

# ----------------------------------------------
# üëá –°–Æ–î–ê –í–°–¢–ê–í–¨ –°–í–û–ô API-—Ç–æ–∫–µ–Ω Hugging Face
HUGGINGFACE_TOKEN = "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"  # üîê –í–°–¢–ê–í–¨ –°–Æ–î–ê —Å–≤–æ–π —Ç–æ–∫–µ–Ω Hugging Face
API_URL = "https://api-inference.huggingface.co/models/mistralai/Mistral-7B-Instruct"
HEADERS = {"Authorization": f"Bearer {HUGGINGFACE_TOKEN}"}
# ----------------------------------------------

st.set_page_config(page_title="ü§ñ –ê—Å—Å–∏—Å—Ç–µ–Ω—Ç + –ü–æ–∏—Å–∫", layout="centered")
st.title("üí¨ –ß–∞—Ç-–ø–æ–º–æ—â–Ω–∏–∫ —Å —É–º–Ω—ã–º –ø–æ–∏—Å–∫–æ–º")

@st.cache_data
def get_data():
    df = load_all_excels()
    from utils import get_model
    model = get_model()
    df.attrs['phrase_embs'] = model.encode(df['phrase_proc'].tolist(), convert_to_tensor=True)
    return df

@st.cache_resource
def query_llm(prompt):
    payload = {"inputs": prompt}
    try:
        response = requests.post(API_URL, headers=HEADERS, json=payload, timeout=30)
        response.raise_for_status()
        return response.json()[0]["generated_text"]
    except Exception as e:
        return f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞—â–µ–Ω–∏–∏ –∫ –º–æ–¥–µ–ª–∏: {e}"

df = get_data()

if "messages" not in st.session_state:
    st.session_state.messages = []

# –ü–æ–∫–∞–∑–∞—Ç—å –∏—Å—Ç–æ—Ä–∏—é
for msg in st.session_state.messages:
    with st.chat_message(msg["role"]):
        st.markdown(msg["content"])

user_prompt = st.chat_input("–í–≤–µ–¥–∏—Ç–µ –≤–∞—à –≤–æ–ø—Ä–æ—Å –∏–ª–∏ —Ñ—Ä–∞–∑—É...")

if user_prompt:
    # –û—Ç–æ–±—Ä–∞–∂–∞–µ–º –≤–≤–æ–¥
    st.chat_message("user").markdown(user_prompt)
    st.session_state.messages.append({"role": "user", "content": user_prompt})

    # –û—Ç–≤–µ—Ç –±–æ—Ç–∞
    with st.chat_message("assistant"):
        with st.spinner("–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞..."):
            reply = query_llm(user_prompt)
            st.markdown(f"**GPT-–±–æ—Ç:** {reply}")
            st.session_state.messages.append({"role": "assistant", "content": f"GPT-–±–æ—Ç: {reply}"})

        # –ü–æ–∏—Å–∫ –ø–æ—Ö–æ–∂–∏—Ö —Ñ—Ä–∞–∑
        with st.spinner("–ü–æ–∏—Å–∫ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö —Ñ—Ä–∞–∑..."):
            results = semantic_search(user_prompt, df)
            if results:
                st.markdown("### üîç –ü–æ—Ö–æ–∂–∏–µ —Ñ—Ä–∞–∑—ã –∏–∑ –±–∞–∑—ã:")
                for score, phrase_full, topics, comment in results:
                    st.markdown(
                        f"""
                        <div style="border:1px solid #ddd; border-radius:10px; padding:10px; margin:10px 0; background:#f9f9f9;">
                        <strong>{phrase_full}</strong><br>
                        üîñ <i>{", ".join(topics)}</i><br>
                        üéØ –†–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å: {score:.2f}
                        </div>
                        """,
                        unsafe_allow_html=True
                    )
            else:
                st.info("–°–æ–≤–ø–∞–¥–µ–Ω–∏–π –≤ –±–∞–∑–µ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ.")
